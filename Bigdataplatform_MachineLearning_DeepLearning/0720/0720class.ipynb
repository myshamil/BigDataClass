{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595220441482",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우  \n",
    "케라스  \n",
    "gensim  \n",
    "scikit-learn  \n",
    "nltk  \n",
    "konlpy  \n",
    "jpype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jones', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop', '.']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"\"\"\n",
    "Don't be fooled by the dark sounding name, Mr. Jones Orphanage is as cheery as cheery goes for a pastery shop.\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jones', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop', '.']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(word_tokenize(\"\"\"\n",
    "Don't be fooled by the dark sounding name, Mr. Jones Orphanage is as cheery as cheery goes for a pastery shop.\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jones', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastery', 'shop', '.']\n"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text  import text_to_word_sequence\n",
    "print(word_tokenize(\"\"\"\n",
    "Don't be fooled by the dark sounding name, Mr. Jones Orphanage is as cheery as cheery goes for a pastery shop.\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 토큰화\n",
    "text1 =\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "text2= \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['His barber kept his word.',\n 'But keeping such a huge secret to himself was driving him crazy.',\n 'Finally, the barber went up a mountain and almost to the edge of a cliff.',\n 'He dug a hole in the midst of some reeds.',\n 'He looked about, to mae sure no one was near.']"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text1)\n",
    "sent_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D', 'student', '.']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('I', 'PRP'),\n ('am', 'VBP'),\n ('actively', 'RB'),\n ('looking', 'VBG'),\n ('for', 'IN'),\n ('Ph.D.', 'NNP'),\n ('students', 'NNS'),\n ('.', '.'),\n ('and', 'CC'),\n ('you', 'PRP'),\n ('are', 'VBP'),\n ('a', 'DT'),\n ('Ph.D', 'NNP'),\n ('student', 'NN'),\n ('.', '.')]"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text1))\n",
    "from nltk.tag import pos_tag\n",
    "imsi = word_tokenize(text1)\n",
    "pos_tag(imsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "#이전 Twitter 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['공부', '우리', '취업']"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "okt= Okt()\n",
    "okt.morphs(\"열심히 공부한 우리. 취업에 성공합시다\") # 형태소\n",
    "okt.pos(\"열심히 공부한 우리. 취업에 성공합시다\") \n",
    "okt.nouns(\"열심히 공부한 우리. 취업에 성공합시다\") # 명사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "love\ndy\n"
    }
   ],
   "source": [
    "#형태소 -> 어간(stem 의미), 접사 (부가적의미)\n",
    "#dogs -> dog -s, cat\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wn = WordNetLemmatizer() #표제어 추출 (품사정보 유지))\n",
    "words=['loves', 'dies']\n",
    "print(wn.lemmatize(words[0]))\n",
    "print(wn.lemmatize(words[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "love\ndie\n"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer() #어간 추출 (품사정보 상실)\n",
    "print(ps.stem(words[0]))\n",
    "print(ps.stem(words[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "#ex에 저장된 단어에서 불용어를 제거하고 출력\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(example)\n",
    "result = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "print(word_tokens) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Family', 'is', 'not', 'an', 'important', 'thing', 'It', 's', 'everything']\n"
    }
   ],
   "source": [
    "tok=RegexpTokenizer(\"[\\w]+\") #\\w =[a-zA-Z0-9]\n",
    "print(tok.tokenize(\"Family is not an important thing. It's everything/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
    }
   ],
   "source": [
    "text= sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11개의 문장 각가에 대해, 단어 토큰화를 수행  \n",
    "1. 불용어는 제거    \n",
    "2. 단어길이가 2이하면 제거  \n",
    "3 모든 단어는 소문자화  \n",
    "11개문장에 대한 단어 토큰화 결과를 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']\n['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'the', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'his', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'his', 'barber', 'kept', 'secret', 'but', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "result = []\n",
    "for i in range(0, len(text)):\n",
    "    word_token = word_tokenize(text[i])\n",
    "    for w in word_token:\n",
    "        if w not in stop_words:\n",
    "            if len(w) > 2: \n",
    "                w = w.lower()\n",
    "                result.append(w)\n",
    "print(word_token)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['the', 'secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['his', 'barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['his', 'barber', 'kept', 'secret'], ['but', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "res = []\n",
    "for i in range(0,11):\n",
    "    word_tokens = word_tokenize(text[i])\n",
    "    result = []\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words and len(w)>2:\n",
    "                result.append(w.lower()) \n",
    "    res.append(result)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n3\n"
    }
   ],
   "source": [
    "vocab = {} \n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) \n",
    "    result = []\n",
    "\n",
    "    for word in sentence: \n",
    "        word = word.lower() \n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    sentences.append(result) \n",
    "print(sentences)\n",
    "print(vocab)\n",
    "print(vocab['person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=sorted(vocab.items(),key=lambda x :x[1],reverse=True)\n",
    "#빈도수를 기준으로 내림차순 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
    }
   ],
   "source": [
    "wti={}#word to index\n",
    "i=0\n",
    "for (word,f) in vs:\n",
    "    if f >1: #단어 빈도수가  2 이상인 경우\n",
    "        i+=1\n",
    "        wti[word]=i\n",
    "print(wti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=okt.morphs(\"오전에는 졸았지만, 오후에는 열심히 하겠다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'오전': 0, '에는': 1, '졸았지만': 2, ',': 3, '오후': 4, '열심히': 5, '하겠다': 6}\n"
    }
   ],
   "source": [
    "#토큰 단위로 딕셔너리에 저장\n",
    "word2index={}\n",
    "for v in token:\n",
    "    if v not in word2index.keys(): #v토큰이 해당 키에 없으면\n",
    "        word2index[v]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 0, 0, 1, 0, 0]\n"
    }
   ],
   "source": [
    "def ohe(w, word2index):\n",
    "    ohv =[0]*len(word2index)\n",
    "    idx=word2index[w]\n",
    "    ohv[idx]=1\n",
    "    return ohv\n",
    "\n",
    "print(ohe(\"오후\", word2index)) #결과 [0,0,0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<keras_preprocessing.text.Tokenizer object at 0x00000264989EDD48>\n"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "text =\"나랑 막걸리 마시러 갈래 안주는 파전 갈래 갈래 파전 최고야\"\n",
    "t =Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "t.word_index #가장많이 등장한 단어부터 역순으로 출력\n",
    "t.index_word\n",
    "enc =t.texts_to_sequences([\"술 마시러 갈래 갈거야 말거야 갈래\"])[0]\n",
    "to_categorical(enc) #one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words: 각단어에 인텍스 부여->등장횟수 저장\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['정부', '가', '발표', '하는', '물가상승률', '과', '소비자', '가', '느끼는', '물가상승률', '은', '다르다']"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "#마침표 제거\n",
    "token =re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "#token = re.sub(\"[.]+\",'',\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "# re.sub(패턴, 교체문자, 데이터)\n",
    "# re.sub(\"대한민국\", \"한국\",\"대한민국 우리나라\")\n",
    "token\n",
    "okt.morphs(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'정': 0}\n==================================================\n{'정': 0, '부': 1}\n==================================================\n{'정': 0, '부': 1, '가': 2}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12, '소': 13}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12, '소': 13, '비': 14}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12, '소': 13, '비': 14, '자': 15}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12, '소': 13, '비': 14, '자': 15, '느': 16}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12, '소': 13, '비': 14, '자': 15, '느': 16, '끼': 17}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12, '소': 13, '비': 14, '자': 15, '느': 16, '끼': 17, '은': 18}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12, '소': 13, '비': 14, '자': 15, '느': 16, '끼': 17, '은': 18, '다': 19}\n==================================================\n{'정': 0, '부': 1, '가': 2, ' ': 3, '발': 4, '표': 5, '하': 6, '는': 7, '물': 8, '상': 9, '승': 10, '률': 11, '과': 12, '소': 13, '비': 14, '자': 15, '느': 16, '끼': 17, '은': 18, '다': 19, '르': 20}\n==================================================\n"
    }
   ],
   "source": [
    "bow=[]\n",
    "word2index={}\n",
    "for v in token:\n",
    "    if v not in word2index.keys():\n",
    "        word2index[v]=len(word2index)\n",
    "        # print(word2index)\n",
    "        # print(\"=\"*50)\n",
    "        bow.insert(len(word2index)-1,1)\n",
    "    else:\n",
    "        idx =word2index.get(v)\n",
    "        bow[idx]=bow[idx]+1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-7c71676e7615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'love'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#스탑워즈 추가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vec=CountVectorizer(stop_words=['love']) #스탑워즈 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF(d,t): 특정문서 d에서 특정 단어 ㅅ의 등장횟수  \n",
    "ex) TF (d1, '안녕)=5 -> d1문서에 '안녕'이 5번 등장  \n",
    "\n",
    "DF(t):특정단어 t가 등장한 문서의 개수  \n",
    "단어가 몇 번 등장했는지는 고려하지 않음.  \n",
    "단지 특정 단어 t가 등장한 문서의 개수만 센다. \n",
    "\n",
    "IDF(d,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "res = []\n",
    "for i in range(n):\n",
    "    res.append([])\n",
    "    d = docs[i] #i번 문서\n",
    "    for j in range(len(voc)):\n",
    "        t = voc[j]\n",
    "        res[-1].append(tfv(t,d))\n",
    "mytf=pd.DataFrame(res, columns=voc)\n",
    "mytf # DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 대한 idf값을 출력 하기 \n",
    "res = []\n",
    "for j in range(len(voc)):\n",
    "    t = voc[j]\n",
    "    res.append(idf(t))\n",
    "idf_=pd.DataFrame(res, index=voc, columns=['IDF'])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n): # 문서의 개수 만큼 돌면서~\n",
    "    d = docs[i]\n",
    "    for j in range(len(voc)):\n",
    "        t = voc[j]\n",
    "        print(tfidf(t,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0 1 0 1 0 1 0 2]\n [0 0 1 0 0 0 0 1]\n [1 0 0 0 1 0 1 0]]\n{'you': 7, 'know': 1, 'want': 5, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
    }
   ],
   "source": [
    "corpus=[\n",
    "    'you know I want you love', 'I like you','what should I do'\n",
    "]\n",
    "#DTM\n",
    "vector=CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.         0.43381609 0.         0.43381609 0.         0.43381609\n  0.         0.65985664]\n [0.         0.         0.79596054 0.         0.         0.\n  0.         0.60534851]\n [0.57735027 0.         0.         0.         0.57735027 0.\n  0.57735027 0.        ]]\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv =TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "\n",
    "#문서를 대표하는 단어를 뽑을 수 있다."
   ]
  }
 ]
}